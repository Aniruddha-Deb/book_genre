{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "14636875-dae7-4f64-b68b-2c1f9ab09d7f",
    "_uuid": "e1eb05c8-d7cd-4839-8085-99a5f10ecc27",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "2eafa9f1-c7b9-40ef-9bdf-6e4da179102e",
    "_uuid": "9bafc1f1-e761-4c94-b3e2-f292655dae93",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /roberta-large/resolve/main/vocab.json (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 302 Moved Temporarily')))' thrown while requesting HEAD https://huggingface.co/roberta-large/resolve/main/vocab.json\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /roberta-large/resolve/main/vocab.json (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 302 Moved Temporarily')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhttp_tunnel_required\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;31m# self._tunnel_host below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0;31m# Mark this connection as not reusable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m             raise OSError(\"Tunnel connection failed: %d %s\" % (code,\n\u001b[0;32m--> 927\u001b[0;31m                                                                message.strip()))\n\u001b[0m\u001b[1;32m    928\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Tunnel connection failed: 302 Moved Temporarily",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 788\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /roberta-large/resolve/main/vocab.json (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 302 Moved Temporarily')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.3308369.pbshpc/ipykernel_31377/373241694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstopping_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRobertaForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                     \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                     \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m                     \u001b[0m_commit_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m                 )\n\u001b[1;32m   1752\u001b[0m                 \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_commit_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         )\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m                 )\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m     )\n\u001b[1;32m   1374\u001b[0m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnb_tries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Sleep for X seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    585\u001b[0m         }\n\u001b[1;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ProxyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProxyError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /roberta-large/resolve/main/vocab.json (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 302 Moved Temporarily')))"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_epochs = 10\n",
    "batch_patience = 4\n",
    "train_batch_size = 32\n",
    "inference_batch_size = 128\n",
    "\n",
    "stopping_stat = 'loss'\n",
    "\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = transformers.RobertaForSequenceClassification.from_pretrained('roberta-large')\n",
    "model.classifier.out_proj = nn.Linear(in_features=1024, out_features=30, bias=True)\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model = model.to(device)\n",
    "\n",
    "train_optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "finetune_optimizer = train_optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# train_steps = (6*34000)/32 ~ 6375.\n",
    "scheduler = None # transformers.get_scheduler(\"linear\", optimizer=train_optimizer, num_warmup_steps=100, num_training_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a3b6133a-2ccd-4b0d-a415-8626760ffbcd",
    "_uuid": "9dca851c-cb1f-4125-b0bc-ec312a8227e1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:59:03.337120Z",
     "iopub.status.busy": "2022-12-18T14:59:03.336713Z",
     "iopub.status.idle": "2022-12-18T14:59:03.342696Z",
     "shell.execute_reply": "2022-12-18T14:59:03.341474Z",
     "shell.execute_reply.started": "2022-12-18T14:59:03.337084Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "337762c5-b86c-4a7e-920a-3dd0d5588da2",
    "_uuid": "313b8a51-a05b-4118-8983-a8a128c99c66",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:59:03.530674Z",
     "iopub.status.busy": "2022-12-18T14:59:03.529940Z",
     "iopub.status.idle": "2022-12-18T14:59:03.537149Z",
     "shell.execute_reply": "2022-12-18T14:59:03.535980Z",
     "shell.execute_reply.started": "2022-12-18T14:59:03.530638Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BookTitleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        record = self.df.iloc[idx]\n",
    "        if self.test:\n",
    "            return (idx, record['Title'])\n",
    "        return (idx, record['Title'], record['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97caf64a-1193-40f0-babd-5b50d711abb3",
    "_uuid": "2ba58410-e79f-4cd2-b2df-c9f6aea16be6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:59:04.968536Z",
     "iopub.status.busy": "2022-12-18T14:59:04.967750Z",
     "iopub.status.idle": "2022-12-18T14:59:05.011339Z",
     "shell.execute_reply": "2022-12-18T14:59:05.010238Z",
     "shell.execute_reply.started": "2022-12-18T14:59:04.968490Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, lr_scheduler, iterator, batch_lim=128):\n",
    "    \n",
    "    model.train()\n",
    "    loss, accuracy = 0, 0\n",
    "    n = batch_lim*train_batch_size\n",
    "    n_steps  = 0\n",
    "    break_early = False\n",
    "    \n",
    "    for i in tqdm(range(batch_lim)):\n",
    "        batch = next(iterator, None)\n",
    "        if batch is None:\n",
    "            break_early = True\n",
    "            break\n",
    "        idxs, titles, genres = batch\n",
    "        tok_titles = tokenizer(list(titles), padding=True, truncation=True, return_tensors='pt')\n",
    "        tok_titles = {k : v.to(device) for k,v in tok_titles.items()}        \n",
    "        genres = genres.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**tok_titles).logits\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        batch_loss = loss_fn(outputs, genres)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.detach()\n",
    "        accuracy += torch.count_nonzero(preds.detach() == genres)\n",
    "\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "                \n",
    "    return {\n",
    "        'loss': loss.cpu().item()/n,\n",
    "        'error': 1 - accuracy.cpu().item()/n,\n",
    "        'break_early': break_early\n",
    "    }\n",
    "    \n",
    "def val_model(model, dataloader):\n",
    "    model.eval()\n",
    "    loss, accuracy = 0, 0\n",
    "    n = len(dataloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            idxs, titles, genres = batch\n",
    "            tok_titles = tokenizer(list(titles), padding=True, truncation=True, return_tensors='pt')\n",
    "            tok_titles = {k : v.to(device) for k,v in tok_titles.items()}        \n",
    "            genres = genres.to(device)\n",
    "\n",
    "            outputs = model(**tok_titles).logits\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            batch_loss = loss_fn(outputs, genres)\n",
    "\n",
    "            loss += batch_loss\n",
    "            accuracy += torch.count_nonzero(preds == genres)\n",
    "        \n",
    "    return {\n",
    "        'loss': loss.cpu().item()/n,\n",
    "        'error': 1 - accuracy.cpu().item()/n\n",
    "    }\n",
    "\n",
    "def predict_model(model, dataloader):\n",
    "    loss, accuracy = 0, 0\n",
    "    n = len(dataloader.dataset)\n",
    "    genres = torch.zeros(n, dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            idxs, titles = batch\n",
    "            tok_titles = tokenizer(list(titles), padding=True, truncation=True, return_tensors='pt')\n",
    "            tok_titles = {k : v.to(device) for k,v in tok_titles.items()}        \n",
    "\n",
    "            outputs = model(**tok_titles).logits\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            genres[idxs] = preds\n",
    "        \n",
    "    return pd.DataFrame({'Genre': genres.cpu()}).rename_axis('Id')\n",
    "\n",
    "def compute_total_norm():\n",
    "    with torch.no_grad():\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "    return total_norm\n",
    "\n",
    "def run(dataloaders, savepath='/kaggle/working'):\n",
    "    \n",
    "    train_stats, val_stats = [], []\n",
    "    \n",
    "    best_stat = 1e15\n",
    "    patience_ctr = 0\n",
    "    best_wts = None\n",
    "    orig_wts = copy.deepcopy(model.state_dict())\n",
    "    training_done = False\n",
    "    \n",
    "    print(f\"Training Model\")\n",
    "    for i in range(1, n_epochs+1):\n",
    "        print(f\"\\nEpoch {i}:\")\n",
    "        \n",
    "        if training_done:\n",
    "            break\n",
    "        \n",
    "        train_iterator = iter(dataloaders['train'])\n",
    "        broke_early = False\n",
    "        while not broke_early:\n",
    "            train_stat = train_model(model, train_optimizer, scheduler, train_iterator)\n",
    "            broke_early = train_stat['break_early']\n",
    "            print(\"  Training:\")\n",
    "            for stat, value in train_stat.items():\n",
    "                print(f\"    {stat:6} = {value}\")\n",
    "\n",
    "            val_stat = val_model(model, dataloaders['val'])\n",
    "            print(\"  Validation:\")\n",
    "            for stat, value in val_stat.items():\n",
    "                print(f\"    {stat:6} = {value}\")\n",
    "\n",
    "            total_norm = compute_total_norm()\n",
    "            train_stat['norm'] = total_norm\n",
    "\n",
    "            train_stats.append(train_stat)\n",
    "            val_stats.append(val_stat)\n",
    "\n",
    "            if (val_stat[stopping_stat] < best_stat):\n",
    "                best_stat = val_stat[stopping_stat]\n",
    "                best_wts = copy.deepcopy(model.state_dict())\n",
    "                patience_ctr = 0\n",
    "            else:\n",
    "                patience_ctr += 1\n",
    "                if patience_ctr >= batch_patience:\n",
    "                    print(f\"{stopping_stat} has not improved in {batch_patience} epochs. Stopping.\")\n",
    "                    training_done = True\n",
    "                    break\n",
    "    \n",
    "    model.load_state_dict(best_wts)\n",
    "    \n",
    "    # finetuning - early stopping here as well?\n",
    "    print(f\"Finetuning Model\")\n",
    "    finetuning_done = False\n",
    "    for i in range(1, n_epochs+1):\n",
    "        print(f\"Epoch {i}:\")\n",
    "        \n",
    "        if finetuning_done:\n",
    "            break\n",
    "        \n",
    "        finetune_iterator = iter(dataloaders['all'])\n",
    "        broke_early = False\n",
    "        while not broke_early:\n",
    "            train_stat = train_model(model, train_optimizer, None, finetune_iterator)\n",
    "            broke_early = train_stat['break_early']\n",
    "            print(\"  Training:\")\n",
    "            for stat, value in train_stat.items():\n",
    "                print(f\"    {stat:6} = {value}\")\n",
    "\n",
    "            val_stat = val_model(model, dataloaders['val'])\n",
    "            print(\"  Validation:\")\n",
    "            for stat, value in val_stat.items():\n",
    "                print(f\"    {stat:6} = {value}\")\n",
    "\n",
    "            total_norm = compute_total_norm()\n",
    "            train_stat['norm'] = total_norm\n",
    "\n",
    "            train_stats.append(train_stat)\n",
    "            val_stats.append(val_stat)\n",
    "\n",
    "            if (val_stat[stopping_stat] < best_stat):\n",
    "                finetuning_done = True\n",
    "                break\n",
    "\n",
    "    torch.save(model, f'{savepath}/roberta.pt')\n",
    "    \n",
    "    return train_stats, val_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a46a391-ac6f-4af4-84cb-880690eed3f5",
    "_uuid": "02c6cc51-ad6d-45a3-811a-572cd1f6f2c5"
   },
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3670e5ac-529f-4107-aaf2-53b4b3f8e0c8",
    "_uuid": "9d90f034-bcee-43db-9a7f-73121fe2667e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:59:05.015191Z",
     "iopub.status.busy": "2022-12-18T14:59:05.014503Z",
     "iopub.status.idle": "2022-12-18T14:59:05.028185Z",
     "shell.execute_reply": "2022-12-18T14:59:05.027094Z",
     "shell.execute_reply.started": "2022-12-18T14:59:05.015150Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df['Title'] = df['Title'].str.strip()\n",
    "    df['Title'] = df['Title'].str.replace(r' [;,\\.:] ', ' ', regex=True)\n",
    "    df['Title'] = df['Title'].str.replace(r'^[;,\\.:]', '', regex=True)\n",
    "    df['Title'] = df['Title'].str.replace(r'[;,\\.:]$', '', regex=True)\n",
    "    df['Title'] = df['Title'].str.strip()\n",
    "    return df\n",
    "\n",
    "def load_train_df(x_path, y_path, debug_len=64):\n",
    "    X = pd.read_csv(x_path)\n",
    "    y = pd.read_csv(y_path)\n",
    "    df = pd.merge(process_df(X), y, left_on='Id', right_on='Id', how='left')\n",
    "    df.set_index('Id', inplace=True)\n",
    "    return df.iloc[:debug_len] if DEBUG else df\n",
    "\n",
    "def load_df(df_path, debug_len=64):\n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df.set_index('Id')\n",
    "    return df.iloc[:debug_len] if DEBUG else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48f22815-6882-4f38-8482-8e39f71dafaa",
    "_uuid": "ab0251e3-9d8e-4df2-9154-033b2f82366e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:59:05.030873Z",
     "iopub.status.busy": "2022-12-18T14:59:05.030527Z",
     "iopub.status.idle": "2022-12-18T14:59:05.374019Z",
     "shell.execute_reply": "2022-12-18T14:59:05.372991Z",
     "shell.execute_reply.started": "2022-12-18T14:59:05.030837Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dpath = '/kaggle/input/col774-2022'\n",
    "train_df = load_train_df(f'{dpath}/train_x.csv', f'{dpath}/train_y.csv', debug_len=128)\n",
    "val_df = load_train_df(f'{dpath}/non_comp_test_x.csv', f'{dpath}/non_comp_test_y.csv', debug_len=32)\n",
    "# gen_df = load_df('/kaggle/input/book-title-generator-data/gen_df.csv')\n",
    "test_df = process_df(load_df(f'{dpath}/comp_test_x.csv'))\n",
    "\n",
    "all_df = train_df.append(val_df, ignore_index=True)\n",
    "\n",
    "train_df, val_df = np.split(all_df.sample(frac=1), [(int)(0.95*len(all_df))])\n",
    "                                  \n",
    "# train_df = train_df.append(gen_df.iloc[:-6000], ignore_index=True)\n",
    "# gen_df = gen_df.iloc[-6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2d7e3250-9d58-458c-b053-1e0d351a05ea",
    "_uuid": "1b633898-c344-47a5-b195-3915e5c8264a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:59:05.375853Z",
     "iopub.status.busy": "2022-12-18T14:59:05.375480Z",
     "iopub.status.idle": "2022-12-18T14:59:05.382357Z",
     "shell.execute_reply": "2022-12-18T14:59:05.381094Z",
     "shell.execute_reply.started": "2022-12-18T14:59:05.375815Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': DataLoader(BookTitleDataset(train_df), batch_size=train_batch_size, shuffle=True, num_workers=2),\n",
    "    'val'  : DataLoader(BookTitleDataset(val_df), batch_size=inference_batch_size, shuffle=False, num_workers=2),\n",
    "    'all'  : DataLoader(BookTitleDataset(all_df), batch_size=train_batch_size, shuffle=True, num_workers=2),\n",
    "    'test' : DataLoader(BookTitleDataset(test_df, test=True), batch_size=inference_batch_size, shuffle=False, num_workers=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b791230-7177-4d62-a1da-545d492b12eb",
    "_uuid": "5aa8a7c5-8ef3-4807-991f-83657faa6c95",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T15:02:00.823176Z",
     "iopub.status.busy": "2022-12-18T15:02:00.822727Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_stats, val_stats = run(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22cf67cf-da90-4fb8-add1-5ee31d6626d7",
    "_uuid": "efb2b8c1-38f1-4493-be81-867541c95223",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T15:01:53.967004Z",
     "iopub.status.busy": "2022-12-18T15:01:53.966512Z",
     "iopub.status.idle": "2022-12-18T15:01:53.976118Z",
     "shell.execute_reply": "2022-12-18T15:01:53.974874Z",
     "shell.execute_reply.started": "2022-12-18T15:01:53.966960Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.module.classifier.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e459cf10-77bb-4bba-8847-f57a1584945b",
    "_uuid": "65bf0a51-5514-48ea-8f7b-494e684e291c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:30:37.861741Z",
     "iopub.status.busy": "2022-12-18T14:30:37.860918Z",
     "iopub.status.idle": "2022-12-18T14:30:38.063862Z",
     "shell.execute_reply": "2022-12-18T14:30:38.062881Z",
     "shell.execute_reply.started": "2022-12-18T14:30:37.861698Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([a['loss'] for a in train_stats], label='Train loss')\n",
    "plt.plot([a['loss'] for a in val_stats], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss v/s epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "94f9454a-eb98-4dbd-b2f7-c89a311e8bb1",
    "_uuid": "fd88fc54-91e9-456f-9797-a7f597a9551c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:30:45.416607Z",
     "iopub.status.busy": "2022-12-18T14:30:45.416240Z",
     "iopub.status.idle": "2022-12-18T14:30:45.606911Z",
     "shell.execute_reply": "2022-12-18T14:30:45.605834Z",
     "shell.execute_reply.started": "2022-12-18T14:30:45.416576Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([a['error'] for a in train_stats], label='Train error')\n",
    "plt.plot([a['error'] for a in val_stats], label='Validation error')\n",
    "plt.legend()\n",
    "plt.title(\"Error v/s epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2bff5f01-b09f-47a1-b28f-55cb9f3523d8",
    "_uuid": "5f74b879-750d-452b-b633-97c94afa61e7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:30:56.128346Z",
     "iopub.status.busy": "2022-12-18T14:30:56.127957Z",
     "iopub.status.idle": "2022-12-18T14:30:56.305779Z",
     "shell.execute_reply": "2022-12-18T14:30:56.304837Z",
     "shell.execute_reply.started": "2022-12-18T14:30:56.128314Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([a['norm'] for a in train_stats], label='norm')\n",
    "plt.title(\"Gradient norm v/s epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "72db5710-1f5b-46ef-9956-0762bf8046d4",
    "_uuid": "64469c19-86ee-416b-b85e-503036e74e69",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:31:01.576225Z",
     "iopub.status.busy": "2022-12-18T14:31:01.574985Z",
     "iopub.status.idle": "2022-12-18T14:31:20.633993Z",
     "shell.execute_reply": "2022-12-18T14:31:20.632964Z",
     "shell.execute_reply.started": "2022-12-18T14:31:01.576178Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "preds = predict_model(model, dataloaders['test'])\n",
    "preds.to_csv(f\"/kaggle/working/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7fe3ce94-7ddc-4aaa-b0b1-b907425869a1",
    "_uuid": "d0321293-c7ac-4faa-940c-aea24e988e89",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "14636875-dae7-4f64-b68b-2c1f9ab09d7f",
    "_uuid": "e1eb05c8-d7cd-4839-8085-99a5f10ecc27",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/cse/btech/cs1200869/book_cover', '/home/cse/btech/cs1200869/.conda/envs/dl_35/lib/python37.zip', '/home/cse/btech/cs1200869/.conda/envs/dl_35/lib/python3.7', '/home/cse/btech/cs1200869/.conda/envs/dl_35/lib/python3.7/lib-dynload', '', '/home/cse/btech/cs1200869/.local/lib/python3.7/site-packages', '/home/cse/btech/cs1200869/.conda/envs/dl_35/lib/python3.7/site-packages', '/home/cse/btech/cs1200869/.conda/envs/dl_35/lib/python3.7/site-packages/IPython/extensions', '/home/cse/btech/cs1200869/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu102\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/cse/btech/cs1200869/.conda/envs/dl_35:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main  \n",
      "_openmp_mutex             5.1                       1_gnu  \n",
      "anyio                     3.5.0            py37h06a4308_0  \n",
      "argon2-cffi               21.3.0             pyhd3eb1b0_0  \n",
      "argon2-cffi-bindings      21.2.0           py37h7f8727e_0  \n",
      "attrs                     22.1.0           py37h06a4308_0  \n",
      "babel                     2.9.1              pyhd3eb1b0_0  \n",
      "backcall                  0.2.0              pyhd3eb1b0_0  \n",
      "beautifulsoup4            4.11.1           py37h06a4308_0  \n",
      "blas                      1.0                         mkl  \n",
      "bleach                    4.1.0              pyhd3eb1b0_0  \n",
      "bottleneck                1.3.5            py37h7deecbd_0  \n",
      "brotli                    1.0.9                h5eee18b_7  \n",
      "brotli-bin                1.0.9                h5eee18b_7  \n",
      "brotlipy                  0.7.0           py37h27cfd23_1003  \n",
      "bzip2                     1.0.8                h7b6447c_0  \n",
      "ca-certificates           2022.10.11           h06a4308_0  \n",
      "certifi                   2022.9.24        py37h06a4308_0  \n",
      "cffi                      1.15.1           py37h5eee18b_3  \n",
      "charset-normalizer        2.0.4              pyhd3eb1b0_0  \n",
      "cryptography              38.0.1           py37h9ce1e76_0  \n",
      "cudatoolkit               10.2.89              hfd86e86_1  \n",
      "cycler                    0.11.0             pyhd3eb1b0_0  \n",
      "dbus                      1.13.18              hb2f20db_0  \n",
      "debugpy                   1.5.1            py37h295c915_0  \n",
      "decorator                 5.1.1              pyhd3eb1b0_0  \n",
      "defusedxml                0.7.1              pyhd3eb1b0_0  \n",
      "entrypoints               0.4              py37h06a4308_0  \n",
      "expat                     2.4.9                h6a678d5_0  \n",
      "ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
      "fftw                      3.3.9                h27cfd23_1  \n",
      "filelock                  3.6.0              pyhd3eb1b0_0  \n",
      "flit-core                 3.6.0              pyhd3eb1b0_0  \n",
      "fontconfig                2.14.1               h52c9d5c_1  \n",
      "fonttools                 4.25.0             pyhd3eb1b0_0  \n",
      "freetype                  2.12.1               h4a9f257_0  \n",
      "giflib                    5.2.1                h7b6447c_0  \n",
      "glib                      2.69.1               he621ea3_2  \n",
      "gmp                       6.2.1                h295c915_3  \n",
      "gnutls                    3.6.15               he1e5248_0  \n",
      "gst-plugins-base          1.14.0               h8213a91_2  \n",
      "gstreamer                 1.14.0               h28cd5cc_2  \n",
      "huggingface_hub           0.10.1           py37h06a4308_0  \n",
      "icu                       58.2                 he6710b0_3  \n",
      "idna                      3.4              py37h06a4308_0  \n",
      "importlib-metadata        4.11.3           py37h06a4308_0  \n",
      "importlib_metadata        4.11.3               hd3eb1b0_0  \n",
      "importlib_resources       5.2.0              pyhd3eb1b0_1  \n",
      "intel-openmp              2021.4.0          h06a4308_3561  \n",
      "ipykernel                 6.15.2           py37h06a4308_0  \n",
      "ipython                   7.31.1           py37h06a4308_1  \n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \n",
      "ipywidgets                7.6.5              pyhd3eb1b0_1  \n",
      "jedi                      0.18.1           py37h06a4308_1  \n",
      "jinja2                    3.1.2            py37h06a4308_0  \n",
      "joblib                    1.1.1            py37h06a4308_0  \n",
      "jpeg                      9e                   h7f8727e_0  \n",
      "json5                     0.9.6              pyhd3eb1b0_0  \n",
      "jsonschema                4.16.0           py37h06a4308_0  \n",
      "jupyter                   1.0.0            py37h06a4308_8  \n",
      "jupyter_client            7.4.7            py37h06a4308_0  \n",
      "jupyter_console           6.4.3              pyhd3eb1b0_0  \n",
      "jupyter_core              4.11.2           py37h06a4308_0  \n",
      "jupyter_server            1.18.1           py37h06a4308_0  \n",
      "jupyterlab                3.5.0            py37h06a4308_0  \n",
      "jupyterlab_pygments       0.1.2                      py_0  \n",
      "jupyterlab_server         2.16.3           py37h06a4308_0  \n",
      "jupyterlab_widgets        1.0.0              pyhd3eb1b0_1  \n",
      "kiwisolver                1.4.2            py37h295c915_0  \n",
      "krb5                      1.19.2               hac12032_0  \n",
      "lame                      3.100                h7b6447c_0  \n",
      "lcms2                     2.12                 h3be6417_0  \n",
      "ld_impl_linux-64          2.38                 h1181459_1  \n",
      "lerc                      3.0                  h295c915_0  \n",
      "libbrotlicommon           1.0.9                h5eee18b_7  \n",
      "libbrotlidec              1.0.9                h5eee18b_7  \n",
      "libbrotlienc              1.0.9                h5eee18b_7  \n",
      "libclang                  10.0.1          default_hb85057a_2  \n",
      "libdeflate                1.8                  h7f8727e_5  \n",
      "libedit                   3.1.20221030         h5eee18b_0  \n",
      "libevent                  2.1.12               h8f2d780_0  \n",
      "libffi                    3.4.2                h6a678d5_6  \n",
      "libgcc-ng                 11.2.0               h1234567_1  \n",
      "libgfortran-ng            11.2.0               h00389a5_1  \n",
      "libgfortran5              11.2.0               h1234567_1  \n",
      "libgomp                   11.2.0               h1234567_1  \n",
      "libiconv                  1.16                 h7f8727e_2  \n",
      "libidn2                   2.3.2                h7f8727e_0  \n",
      "libllvm10                 10.0.1               hbcb73fb_5  \n",
      "libpng                    1.6.37               hbc83047_0  \n",
      "libpq                     12.9                 h16c4e8d_3  \n",
      "libsodium                 1.0.18               h7b6447c_0  \n",
      "libstdcxx-ng              11.2.0               h1234567_1  \n",
      "libtasn1                  4.16.0               h27cfd23_0  \n",
      "libtiff                   4.4.0                hecacb30_2  \n",
      "libunistring              0.9.10               h27cfd23_0  \n",
      "libuuid                   1.41.5               h5eee18b_0  \n",
      "libwebp                   1.2.4                h11a3e52_0  \n",
      "libwebp-base              1.2.4                h5eee18b_0  \n",
      "libxcb                    1.15                 h7f8727e_0  \n",
      "libxkbcommon              1.0.1                hfa300c1_0  \n",
      "libxml2                   2.9.14               h74e7548_0  \n",
      "libxslt                   1.1.35               h4e12654_0  \n",
      "lxml                      4.9.1            py37h1edc446_0  \n",
      "lz4-c                     1.9.4                h6a678d5_0  \n",
      "markupsafe                2.1.1            py37h7f8727e_0  \n",
      "matplotlib                3.5.2            py37h06a4308_0  \n",
      "matplotlib-base           3.5.2            py37hf590b9c_0  \n",
      "matplotlib-inline         0.1.6            py37h06a4308_0  \n",
      "mistune                   0.8.4           py37h14c3975_1001  \n",
      "mkl                       2021.4.0           h06a4308_640  \n",
      "mkl-service               2.4.0            py37h7f8727e_0  \n",
      "mkl_fft                   1.3.1            py37hd3c417c_0  \n",
      "mkl_random                1.2.2            py37h51133e4_0  \n",
      "munkres                   1.1.4                      py_0  \n",
      "nbclassic                 0.4.8            py37h06a4308_0  \n",
      "nbclient                  0.5.13           py37h06a4308_0  \n",
      "nbconvert                 6.5.4            py37h06a4308_0  \n",
      "nbformat                  5.7.0            py37h06a4308_0  \n",
      "ncurses                   6.3                  h5eee18b_3  \n",
      "nest-asyncio              1.5.5            py37h06a4308_0  \n",
      "nettle                    3.7.3                hbbd107a_1  \n",
      "notebook                  6.5.2            py37h06a4308_0  \n",
      "notebook-shim             0.2.2            py37h06a4308_0  \n",
      "nspr                      4.33                 h295c915_0  \n",
      "nss                       3.74                 h0370c37_0  \n",
      "numexpr                   2.8.4            py37he184ba9_0  \n",
      "numpy                     1.21.5           py37h6c91a56_3  \n",
      "numpy-base                1.21.5           py37ha15fc14_3  \n",
      "openh264                  2.1.1                h4ff587b_0  \n",
      "openssl                   1.1.1s               h7f8727e_0  \n",
      "packaging                 21.3               pyhd3eb1b0_0  \n",
      "pandas                    1.3.5            py37h8c16a72_0  \n",
      "pandocfilters             1.5.0              pyhd3eb1b0_0  \n",
      "parso                     0.8.3              pyhd3eb1b0_0  \n",
      "pcre                      8.45                 h295c915_0  \n",
      "pexpect                   4.8.0              pyhd3eb1b0_3  \n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003  \n",
      "pillow                    9.2.0            py37hace64e9_1  \n",
      "pip                       22.3.1           py37h06a4308_0  \n",
      "pkgutil-resolve-name      1.3.10           py37h06a4308_0  \n",
      "ply                       3.11                     py37_0  \n",
      "prometheus_client         0.14.1           py37h06a4308_0  \n",
      "prompt-toolkit            3.0.20             pyhd3eb1b0_0  \n",
      "prompt_toolkit            3.0.20               hd3eb1b0_0  \n",
      "psutil                    5.9.0            py37h5eee18b_0  \n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2  \n",
      "pycparser                 2.21               pyhd3eb1b0_0  \n",
      "pygments                  2.11.2             pyhd3eb1b0_0  \n",
      "pyopenssl                 22.0.0             pyhd3eb1b0_0  \n",
      "pyparsing                 3.0.9            py37h06a4308_0  \n",
      "pyqt                      5.15.7           py37h6a678d5_1  \n",
      "pyqt5-sip                 12.11.0          py37h6a678d5_1  \n",
      "pyrsistent                0.18.0           py37heee7806_0  \n",
      "pysocks                   1.7.1                    py37_1  \n",
      "python                    3.7.15               h7a1cb2a_1  \n",
      "python-dateutil           2.8.2              pyhd3eb1b0_0  \n",
      "python-fastjsonschema     2.16.2           py37h06a4308_0  \n",
      "pytorch                   1.12.1          py3.7_cuda10.2_cudnn7.6.5_0    pytorch\n",
      "pytorch-mutex             1.0                        cuda    pytorch\n",
      "pytz                      2022.1           py37h06a4308_0  \n",
      "pyyaml                    6.0              py37h5eee18b_1  \n",
      "pyzmq                     23.2.0           py37h6a678d5_0  \n",
      "qt-main                   5.15.2               h327a75a_7  \n",
      "qt-webengine              5.15.9               hd2b0992_4  \n",
      "qtconsole                 5.3.2            py37h06a4308_0  \n",
      "qtpy                      2.2.0            py37h06a4308_0  \n",
      "qtwebkit                  5.212                h4eab89a_4  \n",
      "readline                  8.2                  h5eee18b_0  \n",
      "regex                     2022.7.9         py37h5eee18b_0  \n",
      "requests                  2.28.1           py37h06a4308_0  \n",
      "scikit-learn              1.0.2            py37h51133e4_1  \n",
      "scipy                     1.7.3            py37h6c91a56_2  \n",
      "seaborn                   0.12.1           py37h06a4308_0  \n",
      "send2trash                1.8.0              pyhd3eb1b0_1  \n",
      "setuptools                65.5.0           py37h06a4308_0  \n",
      "sip                       6.6.2            py37h6a678d5_0  \n",
      "six                       1.16.0             pyhd3eb1b0_1  \n",
      "sniffio                   1.2.0            py37h06a4308_1  \n",
      "soupsieve                 2.3.2.post1      py37h06a4308_0  \n",
      "sqlite                    3.40.0               h5082296_0  \n",
      "terminado                 0.13.1           py37h06a4308_0  \n",
      "threadpoolctl             2.2.0              pyh0d69192_0  \n",
      "tinycss2                  1.2.1            py37h06a4308_0  \n",
      "tk                        8.6.12               h1ccaba5_0  \n",
      "tokenizers                0.11.4           py37h3dcd8bd_1  \n",
      "toml                      0.10.2             pyhd3eb1b0_0  \n",
      "tomli                     2.0.1            py37h06a4308_0  \n",
      "torchaudio                0.12.1               py37_cu102    pytorch\n",
      "torchvision               0.13.1               py37_cu102    pytorch\n",
      "tornado                   6.2              py37h5eee18b_0  \n",
      "tqdm                      4.64.1           py37h06a4308_0  \n",
      "traitlets                 5.7.1            py37h06a4308_0  \n",
      "transformers              4.24.0           py37h06a4308_0  \n",
      "typing-extensions         4.4.0            py37h06a4308_0  \n",
      "typing_extensions         4.4.0            py37h06a4308_0  \n",
      "urllib3                   1.26.13          py37h06a4308_0  \n",
      "wcwidth                   0.2.5              pyhd3eb1b0_0  \n",
      "webencodings              0.5.1                    py37_1  \n",
      "websocket-client          0.58.0           py37h06a4308_4  \n",
      "wheel                     0.37.1             pyhd3eb1b0_0  \n",
      "widgetsnbextension        3.5.2            py37h06a4308_0  \n",
      "xz                        5.2.8                h5eee18b_0  \n",
      "yaml                      0.2.5                h7b6447c_0  \n",
      "zeromq                    4.3.4                h2531618_0  \n",
      "zipp                      3.8.0            py37h06a4308_0  \n",
      "zlib                      1.2.13               h5eee18b_0  \n",
      "zstd                      1.5.2                ha4553b6_0  \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "2eafa9f1-c7b9-40ef-9bdf-6e4da179102e",
    "_uuid": "9bafc1f1-e761-4c94-b3e2-f292655dae93",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_epochs = 10\n",
    "batch_patience = 4\n",
    "train_batch_size = 32\n",
    "inference_batch_size = 128\n",
    "\n",
    "stopping_stat = 'loss'\n",
    "\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = transformers.RobertaForSequenceClassification.from_pretrained('roberta-large')\n",
    "model.classifier.out_proj = nn.Linear(in_features=1024, out_features=30, bias=True)\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model = model.to(device)\n",
    "\n",
    "train_optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "finetune_optimizer = train_optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# train_steps = (6*34000)/32 ~ 6375.\n",
    "scheduler = None # transformers.get_scheduler(\"linear\", optimizer=train_optimizer, num_warmup_steps=100, num_training_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_35', 'sm_37', 'sm_50', 'sm_60', 'sm_70']\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a3b6133a-2ccd-4b0d-a415-8626760ffbcd",
    "_uuid": "9dca851c-cb1f-4125-b0bc-ec312a8227e1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "337762c5-b86c-4a7e-920a-3dd0d5588da2",
    "_uuid": "313b8a51-a05b-4118-8983-a8a128c99c66",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BookTitleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        record = self.df.iloc[idx]\n",
    "        if self.test:\n",
    "            return (idx, record['Title'])\n",
    "        return (idx, record['Title'], record['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "97caf64a-1193-40f0-babd-5b50d711abb3",
    "_uuid": "2ba58410-e79f-4cd2-b2df-c9f6aea16be6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, lr_scheduler, iterator, batch_lim=128):\n",
    "    \n",
    "    model.train()\n",
    "    loss, accuracy = 0, 0\n",
    "    n = batch_lim*train_batch_size\n",
    "    n_steps  = 0\n",
    "    break_early = False\n",
    "    \n",
    "    for i in tqdm(range(batch_lim)):\n",
    "        batch = next(iterator, None)\n",
    "        if batch is None:\n",
    "            break_early = True\n",
    "            break\n",
    "        idxs, titles, genres = batch\n",
    "        tok_titles = tokenizer(list(titles), padding=True, truncation=True, return_tensors='pt')\n",
    "        tok_titles = {k : v.to(device) for k,v in tok_titles.items()}        \n",
    "        genres = genres.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**tok_titles).logits\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        batch_loss = loss_fn(outputs, genres)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.detach()\n",
    "        accuracy += torch.count_nonzero(preds.detach() == genres)\n",
    "\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "                \n",
    "    return {\n",
    "        'loss': loss.cpu().item()/n,\n",
    "        'error': 1 - accuracy.cpu().item()/n,\n",
    "        'break_early': break_early\n",
    "    }\n",
    "    \n",
    "def val_model(model, dataloader):\n",
    "    model.eval()\n",
    "    loss, accuracy = 0, 0\n",
    "    n = len(dataloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            idxs, titles, genres = batch\n",
    "            tok_titles = tokenizer(list(titles), padding=True, truncation=True, return_tensors='pt')\n",
    "            tok_titles = {k : v.to(device) for k,v in tok_titles.items()}        \n",
    "            genres = genres.to(device)\n",
    "\n",
    "            outputs = model(**tok_titles).logits\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            batch_loss = loss_fn(outputs, genres)\n",
    "\n",
    "            loss += batch_loss\n",
    "            accuracy += torch.count_nonzero(preds == genres)\n",
    "        \n",
    "    return {\n",
    "        'loss': loss.cpu().item()/n,\n",
    "        'error': 1 - accuracy.cpu().item()/n\n",
    "    }\n",
    "\n",
    "def predict_model(model, dataloader):\n",
    "    loss, accuracy = 0, 0\n",
    "    n = len(dataloader.dataset)\n",
    "    genres = torch.zeros(n, dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            idxs, titles = batch\n",
    "            tok_titles = tokenizer(list(titles), padding=True, truncation=True, return_tensors='pt')\n",
    "            tok_titles = {k : v.to(device) for k,v in tok_titles.items()}        \n",
    "\n",
    "            outputs = model(**tok_titles).logits\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            genres[idxs] = preds\n",
    "        \n",
    "    return pd.DataFrame({'Genre': genres.cpu()}).rename_axis('Id')\n",
    "\n",
    "def compute_total_norm():\n",
    "    with torch.no_grad():\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "    return total_norm\n",
    "\n",
    "def run(dataloaders, savepath='/kaggle/working'):\n",
    "    \n",
    "    train_stats, val_stats = [], []\n",
    "    \n",
    "    best_stat = 1e15\n",
    "    patience_ctr = 0\n",
    "    best_wts = None\n",
    "    orig_wts = copy.deepcopy(model.state_dict())\n",
    "    training_done = False\n",
    "    \n",
    "    print(f\"Training Model\")\n",
    "    for i in range(1, n_epochs+1):\n",
    "        print(f\"\\nEpoch {i}:\")\n",
    "        \n",
    "        if training_done:\n",
    "            break\n",
    "        \n",
    "        train_iterator = iter(dataloaders['train'])\n",
    "        broke_early = False\n",
    "        while not broke_early:\n",
    "            train_stat = train_model(model, train_optimizer, scheduler, train_iterator)\n",
    "            broke_early = train_stat['break_early']\n",
    "            print(\"  Training:\")\n",
    "            for stat, value in train_stat.items():\n",
    "                print(f\"    {stat:6} = {value}\")\n",
    "\n",
    "            val_stat = val_model(model, dataloaders['val'])\n",
    "            print(\"  Validation:\")\n",
    "            for stat, value in val_stat.items():\n",
    "                print(f\"    {stat:6} = {value}\")\n",
    "\n",
    "            total_norm = compute_total_norm()\n",
    "            train_stat['norm'] = total_norm\n",
    "\n",
    "            train_stats.append(train_stat)\n",
    "            val_stats.append(val_stat)\n",
    "\n",
    "            if (val_stat[stopping_stat] < best_stat):\n",
    "                best_stat = val_stat[stopping_stat]\n",
    "                best_wts = copy.deepcopy(model.state_dict())\n",
    "                patience_ctr = 0\n",
    "            else:\n",
    "                patience_ctr += 1\n",
    "                if patience_ctr >= batch_patience:\n",
    "                    print(f\"{stopping_stat} has not improved in {batch_patience} epochs. Stopping.\")\n",
    "                    training_done = True\n",
    "                    break\n",
    "    \n",
    "    model.load_state_dict(best_wts)\n",
    "    \n",
    "    # finetuning - early stopping here as well?\n",
    "    print(f\"Finetuning Model\")\n",
    "    finetuning_done = False\n",
    "    for i in range(1, n_epochs+1):\n",
    "        print(f\"Epoch {i}:\")\n",
    "        \n",
    "        if finetuning_done:\n",
    "            break\n",
    "        \n",
    "        finetune_iterator = iter(dataloaders['all'])\n",
    "        broke_early = False\n",
    "        while not broke_early:\n",
    "            train_stat = train_model(model, train_optimizer, None, finetune_iterator)\n",
    "            broke_early = train_stat['break_early']\n",
    "            print(\"  Training:\")\n",
    "            for stat, value in train_stat.items():\n",
    "                print(f\"    {stat:6} = {value}\")\n",
    "\n",
    "            val_stat = val_model(model, dataloaders['val'])\n",
    "            print(\"  Validation:\")\n",
    "            for stat, value in val_stat.items():\n",
    "                print(f\"    {stat:6} = {value}\")\n",
    "\n",
    "            total_norm = compute_total_norm()\n",
    "            train_stat['norm'] = total_norm\n",
    "\n",
    "            train_stats.append(train_stat)\n",
    "            val_stats.append(val_stat)\n",
    "\n",
    "            if (val_stat[stopping_stat] < best_stat):\n",
    "                finetuning_done = True\n",
    "                break\n",
    "\n",
    "    torch.save(model, f'{savepath}/roberta.pt')\n",
    "    \n",
    "    return train_stats, val_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a46a391-ac6f-4af4-84cb-880690eed3f5",
    "_uuid": "02c6cc51-ad6d-45a3-811a-572cd1f6f2c5"
   },
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "3670e5ac-529f-4107-aaf2-53b4b3f8e0c8",
    "_uuid": "9d90f034-bcee-43db-9a7f-73121fe2667e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df['Title'] = df['Title'].str.strip()\n",
    "    df['Title'] = df['Title'].str.replace(r' [;,\\.:] ', ' ', regex=True)\n",
    "    df['Title'] = df['Title'].str.replace(r'^[;,\\.:]', '', regex=True)\n",
    "    df['Title'] = df['Title'].str.replace(r'[;,\\.:]$', '', regex=True)\n",
    "    df['Title'] = df['Title'].str.strip()\n",
    "    return df\n",
    "\n",
    "def load_train_df(x_path, y_path, debug_len=64):\n",
    "    X = pd.read_csv(x_path)\n",
    "    y = pd.read_csv(y_path)\n",
    "    df = pd.merge(process_df(X), y, left_on='Id', right_on='Id', how='left')\n",
    "    df.set_index('Id', inplace=True)\n",
    "    return df.iloc[:debug_len] if DEBUG else df\n",
    "\n",
    "def load_df(df_path, debug_len=64):\n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df.set_index('Id')\n",
    "    return df.iloc[:debug_len] if DEBUG else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "48f22815-6882-4f38-8482-8e39f71dafaa",
    "_uuid": "ab0251e3-9d8e-4df2-9154-033b2f82366e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpath = 'data'\n",
    "train_df = load_train_df(f'{dpath}/train_x.csv', f'{dpath}/train_y.csv', debug_len=128)\n",
    "val_df = load_train_df(f'{dpath}/non_comp_test_x.csv', f'{dpath}/non_comp_test_y.csv', debug_len=32)\n",
    "# gen_df = load_df('/kaggle/input/book-title-generator-data/gen_df.csv')\n",
    "test_df = process_df(load_df(f'{dpath}/comp_test_x.csv'))\n",
    "\n",
    "all_df = train_df.append(val_df, ignore_index=True)\n",
    "\n",
    "train_df, val_df = np.split(all_df.sample(frac=1), [(int)(0.95*len(all_df))])\n",
    "                                  \n",
    "# train_df = train_df.append(gen_df.iloc[:-6000], ignore_index=True)\n",
    "# gen_df = gen_df.iloc[-6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "2d7e3250-9d58-458c-b053-1e0d351a05ea",
    "_uuid": "1b633898-c344-47a5-b195-3915e5c8264a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': DataLoader(BookTitleDataset(train_df), batch_size=train_batch_size, shuffle=True, num_workers=2),\n",
    "    'val'  : DataLoader(BookTitleDataset(val_df), batch_size=inference_batch_size, shuffle=False, num_workers=2),\n",
    "    'all'  : DataLoader(BookTitleDataset(all_df), batch_size=train_batch_size, shuffle=True, num_workers=2),\n",
    "    'test' : DataLoader(BookTitleDataset(test_df, test=True), batch_size=inference_batch_size, shuffle=False, num_workers=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "0b791230-7177-4d62-a1da-545d492b12eb",
    "_uuid": "5aa8a7c5-8ef3-4807-991f-83657faa6c95",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0097cb050a343868f951c0f0350d526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.17 GiB total capacity; 10.29 GiB already allocated; 1.06 MiB free; 10.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.3308645.pbshpc/ipykernel_2155/4112109855.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/pbs.3308645.pbshpc/ipykernel_2155/4095092222.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataloaders, savepath)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mbroke_early\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbroke_early\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mtrain_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mbroke_early\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_stat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'break_early'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Training:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/pbs.3308645.pbshpc/ipykernel_2155/4095092222.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, lr_scheduler, iterator, batch_lim)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtok_titles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m         )\n\u001b[1;32m   1221\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m         )\n\u001b[1;32m    864\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m                 )\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         )\n\u001b[1;32m    420\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         )\n\u001b[1;32m    349\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl_35/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.17 GiB total capacity; 10.29 GiB already allocated; 1.06 MiB free; 10.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_stats, val_stats = run(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22cf67cf-da90-4fb8-add1-5ee31d6626d7",
    "_uuid": "efb2b8c1-38f1-4493-be81-867541c95223",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T15:01:53.967004Z",
     "iopub.status.busy": "2022-12-18T15:01:53.966512Z",
     "iopub.status.idle": "2022-12-18T15:01:53.976118Z",
     "shell.execute_reply": "2022-12-18T15:01:53.974874Z",
     "shell.execute_reply.started": "2022-12-18T15:01:53.966960Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.module.classifier.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e459cf10-77bb-4bba-8847-f57a1584945b",
    "_uuid": "65bf0a51-5514-48ea-8f7b-494e684e291c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:30:37.861741Z",
     "iopub.status.busy": "2022-12-18T14:30:37.860918Z",
     "iopub.status.idle": "2022-12-18T14:30:38.063862Z",
     "shell.execute_reply": "2022-12-18T14:30:38.062881Z",
     "shell.execute_reply.started": "2022-12-18T14:30:37.861698Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([a['loss'] for a in train_stats], label='Train loss')\n",
    "plt.plot([a['loss'] for a in val_stats], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss v/s epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "94f9454a-eb98-4dbd-b2f7-c89a311e8bb1",
    "_uuid": "fd88fc54-91e9-456f-9797-a7f597a9551c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:30:45.416607Z",
     "iopub.status.busy": "2022-12-18T14:30:45.416240Z",
     "iopub.status.idle": "2022-12-18T14:30:45.606911Z",
     "shell.execute_reply": "2022-12-18T14:30:45.605834Z",
     "shell.execute_reply.started": "2022-12-18T14:30:45.416576Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([a['error'] for a in train_stats], label='Train error')\n",
    "plt.plot([a['error'] for a in val_stats], label='Validation error')\n",
    "plt.legend()\n",
    "plt.title(\"Error v/s epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2bff5f01-b09f-47a1-b28f-55cb9f3523d8",
    "_uuid": "5f74b879-750d-452b-b633-97c94afa61e7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:30:56.128346Z",
     "iopub.status.busy": "2022-12-18T14:30:56.127957Z",
     "iopub.status.idle": "2022-12-18T14:30:56.305779Z",
     "shell.execute_reply": "2022-12-18T14:30:56.304837Z",
     "shell.execute_reply.started": "2022-12-18T14:30:56.128314Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([a['norm'] for a in train_stats], label='norm')\n",
    "plt.title(\"Gradient norm v/s epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "72db5710-1f5b-46ef-9956-0762bf8046d4",
    "_uuid": "64469c19-86ee-416b-b85e-503036e74e69",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-18T14:31:01.576225Z",
     "iopub.status.busy": "2022-12-18T14:31:01.574985Z",
     "iopub.status.idle": "2022-12-18T14:31:20.633993Z",
     "shell.execute_reply": "2022-12-18T14:31:20.632964Z",
     "shell.execute_reply.started": "2022-12-18T14:31:01.576178Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "preds = predict_model(model, dataloaders['test'])\n",
    "preds.to_csv(f\"/kaggle/working/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7fe3ce94-7ddc-4aaa-b0b1-b907425869a1",
    "_uuid": "d0321293-c7ac-4faa-940c-aea24e988e89",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
